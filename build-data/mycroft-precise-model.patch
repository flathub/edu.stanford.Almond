From 4e23741227a0db9b9b6d99f84b62d4bde18f7733 Mon Sep 17 00:00:00 2001
From: Giovanni Campagna <gcampagn@cs.stanford.edu>
Date: Mon, 29 Apr 2019 22:24:45 -0700
Subject: [PATCH] Update model

Use a better one, with one extra dense layer between the GRU
and the output, with L2 regularization (weight decay) and with
SDG optimizer with learning rate scheduling.
---
 precise/functions.py | 24 ++++++++++++++++++++++++
 precise/model.py     | 33 ++++++++++++++++++++++++---------
 2 files changed, 48 insertions(+), 9 deletions(-)

diff --git a/precise/functions.py b/precise/functions.py
index 877c14d..7bba240 100644
--- a/precise/functions.py
+++ b/precise/functions.py
@@ -61,12 +61,36 @@ def false_neg(yt, yp) -> Any:
     return K.sum(K.cast((1 - yp) * (0 + yt) > 0.5, 'float')) / K.maximum(1.0, K.sum(0 + yt))
 
 
+def precision(y_gold, y_pred):
+    from keras import backend as K
+    tp = K.sum(K.cast(y_gold * y_pred > 0.5, 'float'))
+    fp = K.sum(K.cast((1 - y_gold) * y_pred > 0.5, 'float'))
+    return tp / K.maximum(1.0, (tp + fp))
+
+
+def recall(y_gold, y_pred):
+    from keras import backend as K
+    tp = K.sum(K.cast(y_gold * y_pred > 0.5, 'float'))
+    fn = K.sum(K.cast(y_gold * (1 - y_pred) > 0.5, 'float'))
+    return tp / K.maximum(1.0, (tp + fn))
+
+
+def f_score(y_gold, y_pred):
+    import tensorflow as tf
+    pr = precision(y_gold, y_pred)
+    re = recall(y_gold, y_pred)
+    return tf.cond((pr + re) > 0.0,
+        lambda: (2 * pr * re) / (pr + re),
+        lambda: 0.0)
+
+
 def load_keras() -> Any:
     import keras
     keras.losses.weighted_log_loss = weighted_log_loss
     keras.metrics.false_pos = false_pos
     keras.metrics.false_positives = false_pos
     keras.metrics.false_neg = false_neg
+    keras.metrics.f_score = f_score
     return keras
 
 
diff --git a/precise/model.py b/precise/model.py
index 6ee90ca..cc8b72e 100644
--- a/precise/model.py
+++ b/precise/model.py
@@ -15,7 +15,7 @@ import attr
 from os.path import isfile
 from typing import *
 
-from precise.functions import load_keras, false_pos, false_neg, weighted_log_loss, set_loss_bias
+from precise.functions import load_keras, false_pos, false_neg, f_score, weighted_log_loss, set_loss_bias
 from precise.params import inject_params, pr
 
 if TYPE_CHECKING:
@@ -31,8 +31,10 @@ class ModelParams:
         extra_metrics: Whether to include false positive and false negative metrics
         skip_acc: Whether to skip accuracy calculation while training
     """
-    recurrent_units = attr.ib(20)  # type: int
-    dropout = attr.ib(0.2)  # type: float
+    recurrent_units = attr.ib(50)  # type: int
+    input_dropout = attr.ib(0.2)  # type: float
+    dropout = attr.ib(0.4) # type: float
+    weight_decay = attr.ib(1e-4) # type: float
     extra_metrics = attr.ib(False)  # type: bool
     skip_acc = attr.ib(False)  # type: bool
     loss_bias = attr.ib(0.7)  # type: float
@@ -63,21 +65,34 @@ def create_model(model_name: Optional[str], params: ModelParams) -> 'Sequential'
         print('Loading from ' + model_name + '...')
         model = load_precise_model(model_name)
     else:
-        from keras.layers.core import Dense
+        from keras.layers.core import Dense, Dropout
         from keras.layers.recurrent import GRU
         from keras.models import Sequential
+        from keras import regularizers
+
+        l2 = regularizers.l2(params.weight_decay)
 
         model = Sequential()
         model.add(GRU(
-            params.recurrent_units, activation='linear',
-            input_shape=(pr.n_features, pr.feature_size), dropout=params.dropout, name='net'
+            params.recurrent_units, activation='tanh',
+            input_shape=(pr.n_features, pr.feature_size), dropout=params.input_dropout, name='gru0',
         ))
-        model.add(Dense(1, activation='sigmoid'))
+        model.add(Dropout(params.dropout))
+        model.add(Dense(params.recurrent_units,
+                        kernel_regularizer=l2,
+                        activation='relu'))
+        model.add(Dropout(params.dropout))
+        model.add(Dense(1,
+                        kernel_regularizer=l2,
+                        activation='sigmoid'))
 
     load_keras()
-    metrics = ['accuracy'] + params.extra_metrics * [false_pos, false_neg]
+    metrics = ['accuracy', f_score] + params.extra_metrics * [false_pos, false_neg]
     set_loss_bias(params.loss_bias)
     for i in model.layers[:params.freeze_till]:
         i.trainable = False
-    model.compile('rmsprop', weighted_log_loss, metrics=(not params.skip_acc) * metrics)
+    from keras import optimizers
+    
+    sgd = optimizers.SGD(lr=1.0, decay=1e-4)
+    model.compile(optimizer=sgd, loss=weighted_log_loss, metrics=(not params.skip_acc) * metrics)
     return model
-- 
2.21.0

